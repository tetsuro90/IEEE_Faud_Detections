{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'drop_features' is running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--config CONFIG]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-7c8365b7-292e-4ea8-94d6-5f537d2431db.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/tmp/working/IEEE_Fraud_Detection/')\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from functions.functions import load_datasets, reduce_mem_usage, relax_data\n",
    "from models.models import train_model_classification\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit, GroupKFold\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "# calculate execution time\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"\\n'drop_features' is running...\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# JSONファイルからのconfigの読み込み\n",
    "parser: ArgumentParser = argparse.ArgumentParser()  # パーサのインスタンス化\n",
    "parser.add_argument('--config', default='./configs/default.json')  # 受け取る引数の追加。引数は--config, デフォルトも設定\n",
    "options = parser.parse_args()  # 解析した引数をoptionsに格納\n",
    "config = json.load(open(options.config))  # 引数に渡したJSONファイルを開く\n",
    "\n",
    "##### logging #####\n",
    "now = datetime.datetime.now()\n",
    "logging.basicConfig(\n",
    "    filename='./logs/drop_feature_log_{0:%Y%m%d%H%M}.log'.format(now), level=logging.DEBUG\n",
    ")\n",
    "logging.debug('./logs/drop_feature_log_{0:%Y%m%d%H%M}.log'.format(now))\n",
    "\n",
    "##### whether use pickle data or not #####\n",
    "logging.debug('\\n\\n=== used feature mode =========')\n",
    "feature_mode = config['feature_mode']\n",
    "logging.debug(feature_mode)\n",
    "\n",
    "\n",
    "# if feature_mode == 'pickle':\n",
    "#     train = pd.read_pickle('train_2.pkl')\n",
    "#     test = pd.read_pickle('test_2.pkl')\n",
    "#     train = train.drop(['uid', 'uid2', 'uid3', 'uid4', 'uid5', 'bank_type'], axis=1)\n",
    "#     test = test.drop(['uid', 'uid2', 'uid3', 'uid4', 'uid5', 'bank_type'], axis=1)\n",
    "#\n",
    "#     # add feature\n",
    "#     logging.debug('\\n\\n=== stacking =========')\n",
    "#     stacking = config['stacking']\n",
    "#     logging.debug(stacking)\n",
    "#     if stacking == 'true':\n",
    "#         train_add = pd.read_csv('nn_oof_train.csv')\n",
    "#         test_add = pd.read_csv('nn_oof_test.csv')\n",
    "#         train['oof'] = train_add['oof']\n",
    "#         test['oof'] = test_add['isFraud']\n",
    "\n",
    "\n",
    "\n",
    "logging.debug('\\n\\n=== features =========')\n",
    "feats = ['transaction_identity_merged', 'd_columns_engineering', 'group_v_pca', 'date_of_month']\n",
    "logging.debug(feats)\n",
    "\n",
    "# featherからデータの読み込み\n",
    "train, test = load_datasets(feats)\n",
    "# Noneをnp.nanに戻す\n",
    "train.replace(to_replace=[None], value=np.nan, inplace=True)\n",
    "test.replace(to_replace=[None], value=np.nan, inplace=True)\n",
    "\n",
    "\n",
    "cols_to_drop=[]\n",
    "cols_to_drop += ['V' + str(i) for i in range(1, 340)]\n",
    "cols_to_drop = list(set(cols_to_drop))\n",
    "cols_to_drop = list(set(cols_to_drop) & set(list(train)))\n",
    "train = train.drop(cols_to_drop, axis=1)\n",
    "test = test.drop(cols_to_drop, axis=1)\n",
    "\n",
    "print('\\nTo drop column : ', cols_to_drop)\n",
    "logging.debug('\\n\\n=== Dropped columns =========')\n",
    "logging.debug(cols_to_drop)\n",
    "\n",
    "# カテゴリカル変数をintへ\n",
    "cat_cols = ['id_12',\n",
    "            'id_13',\n",
    "            'id_14',\n",
    "            'id_15',\n",
    "            'id_16',\n",
    "            'id_17',\n",
    "            'id_18',\n",
    "            'id_19',\n",
    "            'id_20',\n",
    "            'id_21',\n",
    "            'id_22',\n",
    "            'id_23',\n",
    "            'id_24',\n",
    "            'id_25',\n",
    "            'id_26',\n",
    "            'id_27',\n",
    "            'id_28',\n",
    "            'id_29',\n",
    "            'id_30',\n",
    "            'id_31',\n",
    "            'id_32',\n",
    "            'id_33',\n",
    "            'id_34',\n",
    "            'id_35',\n",
    "            'id_36',\n",
    "            'id_37',\n",
    "            'id_38',\n",
    "            'DeviceType',\n",
    "            'DeviceInfo',\n",
    "            'ProductCD',\n",
    "            'card4',\n",
    "            'card6',\n",
    "            'M4',\n",
    "            'P_emaildomain',\n",
    "            'R_emaildomain',\n",
    "            'card1',\n",
    "            'card2',\n",
    "            'card3',\n",
    "            'card5',\n",
    "            'addr1',\n",
    "            'addr2',\n",
    "            'M1',\n",
    "            'M2',\n",
    "            'M3',\n",
    "            'M5',\n",
    "            'M6',\n",
    "            'M7',\n",
    "            'M8',\n",
    "            'M9'\n",
    "            ]\n",
    "\n",
    "\n",
    "########################### Transform Heavy Dominated columns\n",
    "total_items = len(train)\n",
    "keep_cols = ['isFraud']\n",
    "\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col in train.columns:\n",
    "        # train[col] = train[col].fillna('unseen_before_label')\n",
    "        # test[col] = test[col].fillna('unseen_before_label')\n",
    "        train[col] = train[col].astype(str)\n",
    "        test[col] = test[col].astype(str)\n",
    "        le = LabelEncoder()\n",
    "        le.fit(list(train[col]) + list(test[col]))\n",
    "        train[col] = le.transform(list(train[col]))\n",
    "        test[col] = le.transform(list(test[col]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training params\n",
    "model_type = config['model_type']\n",
    "logging.debug('\\n\\n=== model type =========')\n",
    "logging.debug(model_type)\n",
    "\n",
    "params = config['{}_params'.format(model_type)]\n",
    "logging.debug('\\n\\n=== params =========')\n",
    "logging.debug(params)\n",
    "\n",
    "logging.debug('\\n\\n=== Down sampling Rate =========')\n",
    "dwsp_rate = config['downsampling_rate']\n",
    "logging.debug(dwsp_rate)\n",
    "\n",
    "logging.debug('\\n\\n=== Bagging times =========')\n",
    "bag_times = config['bagging_times']\n",
    "logging.debug(bag_times)\n",
    "\n",
    "logging.debug('\\n\\n=== random_seed_average times =========')\n",
    "random_seed_average_times = config['random_seed_average_times']\n",
    "logging.debug(random_seed_average_times)\n",
    "\n",
    "logging.debug('\\n\\n=== N Folds =========')\n",
    "n_fold = config['n_fold']\n",
    "logging.debug(n_fold)\n",
    "\n",
    "logging.debug('\\n\\n=== Folds Type =========')\n",
    "folds_type = {'time_series': TimeSeriesSplit(n_fold), 'k_fold': KFold(n_fold),\n",
    "              'group_k_fold': GroupKFold(n_fold), 'train_test_split_time_series': 'train_test_split_time_series'}\n",
    "folds = folds_type[config['folds_type']]\n",
    "logging.debug(config['folds_type'])\n",
    "if config['folds_type'] == 'group_k_fold':\n",
    "    split_groups = train['DT_M']\n",
    "else:\n",
    "    split_groups = None\n",
    "\n",
    "\n",
    "logging.debug('\\n\\n=== train shape =========')\n",
    "logging.debug(train.shape)\n",
    "print('train shape', train.shape)\n",
    "\n",
    "\n",
    "# Bagging List\n",
    "y_preds = []\n",
    "score_list = []\n",
    "oof_list = []\n",
    "\n",
    "X_test = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)\n",
    "test = test[[\"TransactionDT\", 'TransactionID']]\n",
    "train_fr = train.loc[train['isFraud'] == 1]\n",
    "train_nofr = train.loc[train['isFraud'] == 0]\n",
    "del train\n",
    "train_nofr_dwsp = train_nofr.sample(n=math.floor(train_nofr.shape[0] * dwsp_rate), replace=True, random_state=0)\n",
    "train_dwsp = pd.concat([train_fr, train_nofr_dwsp]).sample(frac=1, random_state=0)\n",
    "X = train_dwsp.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', 'TransactionID'], axis=1)\n",
    "y = train_dwsp.sort_values('TransactionDT')['isFraud']\n",
    "del train_dwsp\n",
    "gc.collect()\n",
    "print('\\n Down sampling is completed.\\n')\n",
    "\n",
    "save_df, save_df_dummy = load_datasets(['d_columns_engineering'])\n",
    "retain_cols = list(save_df)\n",
    "del save_df, save_df_dummy\n",
    "\n",
    "\n",
    "drop_cols = []\n",
    "\n",
    "for col_to_test in retain_cols:\n",
    "    retain_list = [retain for retain in retain_cols if retain not in drop_cols]\n",
    "    retain_list.remove(col_to_test)\n",
    "    print(retain_list)\n",
    "\n",
    "    X_train = X.drop(col_to_test, axis=1)\n",
    "    X_test_test = X_test.drop(col_to_test, axis=1)\n",
    "\n",
    "    cat_cols_temp = list(set(cat_cols) and set(retain_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
